---
title: "analysis"
author: "Yong Lee"
date: "5/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# clear
rm(list = ls())

# load packages 
library(tidyverse)
library(gganimate)
library(rpart)
library(rattle)
library(caret)
library(randomForest)
library(glmnet)
library(ggdendro)
library(patchwork)

# load data
data <- read_rds("shiny/raw-data/data_all.rds")
season_all <- read_rds("shiny/raw-data/season_all.rds")
data_winner <- read_rds("shiny/raw-data/data_winner.rds")
season_winner <- read_rds("shiny/raw-data/season_winner.rds")
season_test <- read_rds("shiny/raw-data/season_test.rds")
tournament_all <- read_rds("shiny/raw-data/tournament_all.rds")
```

```{r visualization}
# group teams by year and threshold parameter
finals <- season_all %>% 
  mutate(threshold = as.factor(ifelse(tour_games >= 6, 1, 0))) %>% 
  select(adjoe:wab, year, win_pct, threshold) %>% 
  group_by(year, threshold) %>% 
  summarize_all(mean) 
saveRDS(finals, "shiny/raw-data/finals.rds")

final_4 <- season_all %>% 
  mutate(threshold = as.factor(ifelse(tour_games >= 5, 1, 0))) %>% 
  select(adjoe:wab, year, win_pct, threshold) %>% 
  group_by(year, threshold) %>% 
  summarize_all(mean)
saveRDS(final_4, "shiny/raw-data/final_4.rds")

elite_8 <- season_all %>% 
  mutate(threshold = as.factor(ifelse(tour_games >= 4, 1, 0))) %>% 
  select(adjoe:wab, year, win_pct, threshold) %>% 
  group_by(year, threshold) %>% 
  summarize_all(mean)
saveRDS(elite_8, "shiny/raw-data/elite_8.rds")

sweet_16 <- season_all %>% 
  mutate(threshold = as.factor(ifelse(tour_games >= 3, 1, 0))) %>% 
  select(adjoe:wab, year, win_pct, threshold) %>% 
  group_by(year, threshold) %>% 
  summarize_all(mean)
saveRDS(sweet_16, "shiny/raw-data/sweet_16.rds")

round_32 <- season_all %>% 
  mutate(threshold = as.factor(ifelse(tour_games >= 2, 1, 0))) %>% 
  select(adjoe:wab, year, win_pct, threshold) %>% 
  group_by(year, threshold) %>% 
  summarize_all(mean)
saveRDS(round_32, "shiny/raw-data/round_32.rds")

# plot comparison between qualifiers and non-qualifiers
finals %>% 
  ggplot(aes(year, win_pct)) + 
  geom_line(aes(group = threshold, color = threshold)) +
  theme_classic()
```

```{r classification_tree}
# assign predictors and outcome to formula variable
formula <- formula('factor(win) ~ win_pct_rank + adjoe_rank + adjde_rank + efg_o_rank + efg_d_rank + tor_rank + tord_rank + orb_rank + drb_rank + ftr_rank + ftrd_rank + twop_o_rank + twop_d_rank + threep_o_rank + threep_d_rank + adj_t_rank + win_pct_rank_opp + adjoe_rank_opp + adjde_rank_opp + efg_o_rank_opp + efg_d_rank_opp + tor_rank_opp + tord_rank_opp + orb_rank_opp + drb_rank_opp + ftr_rank_opp + ftrd_rank_opp + twop_o_rank_opp + twop_d_rank_opp + threep_o_rank_opp + threep_d_rank_opp + adj_t_rank_opp')

# cross-validation for optimal tuning parameter
class_tree <- train(formula, data, method = 'rpart', metric = 'Accuracy', trControl = trainControl(method = "repeatedcv", repeats = 10, number = 20), na.action = na.omit)
class_tree
fancyRpartPlot(class_tree$finalModel)

# single tree is 64% accurate... not great
```

```{r random_forest}
# generate random forest
random_forest <- randomForest(formula, data = data, method = 'rf', ntree = 200, na.action = na.omit)
random_forest
plot(random_forest)
importance(random_forest)

# sample half the observations for testing
data_rows <- sample(nrow(data), floor(nrow(data) * 0.9))
forest_train <- data[data_rows, ]
forest_test <- data[-data_rows, ]

# generate predictions
random_forest <- randomForest(formula, data = forest_train, method = 'rf', ntree = 200, na.action = na.omit)
# test accuracy of predictions using subset of sample
preds <- predict(random_forest, forest_test)

# generate table of prediction accuracy
my_tab <- table(factor(forest_test$win > 0), preds, dnn = c('actual', 'predicted'))
my_tab

# _% of observations accurately predicted... better than single tree
sum(diag(my_tab))/sum(my_tab)
# correctly classified _% of losers ... but only _% of winners
prop.table(my_tab, 1)
# of the teams we thought would win, _% did. Of those we thought would be lose, _% did
prop.table(my_tab, 2) 
```

```{r lasso}
# assign all predictors to list
metrics <- c('win_pct_rank', 'adjoe_rank', 'adjde_rank', 'efg_o_rank', 'efg_d_rank', 'tor_rank', 'tord_rank', 'orb_rank', 'drb_rank', 'ftr_rank', 'ftrd_rank', 'twop_o_rank', 'twop_d_rank', 'threep_o_rank', 'threep_d_rank', 'adj_t_rank', 'win_pct_rank_opp', 'adjoe_rank_opp', 'adjde_rank_opp', 'efg_o_rank_opp', 'efg_d_rank_opp', 'tor_rank_opp', 'tord_rank_opp', 'orb_rank_opp', 'drb_rank_opp', 'ftr_rank_opp', 'ftrd_rank_opp', 'twop_o_rank_opp', 'twop_d_rank_opp', 'threep_o_rank_opp', 'threep_d_rank_opp', 'adj_t_rank_opp')

# specify regression model
model <- paste(c('~ -1', metrics), collapse = ' + ') %>% formula()
x <- model.matrix(model, data = data)

# LASSO with cross-validation
lasso_cv <- cv.glmnet(x = x, y = data$win, alpha = 1, nfolds = 100)
plot(lasso_cv)

# win percentage, offensive efficiency, defensive efficiency, and defensive rebounding were the most leveraged predictors
## defense wins championships?
lasso_cv$lambda.1se 
coef(lasso_cv, s = lasso_cv$lambda.1se)
```

```{r ridge}
# specify regression model
model <- paste(c('~ -1', metrics), collapse = ' + ') %>% formula()
x <- model.matrix(model, data = data)

# cross-validation for tuning parameter
ridge_cv <- cv.glmnet(x = x, y = data$win, alpha = 0, nfolds = 100)
plot(ridge_cv)

# all predictors considered but win percentage, offensive efficiency, defensive efficiency, and defensive rebounding most leveraged
ridge_cv$lambda.1se
coef(ridge_cv, s = ridge_cv$lambda.min)
```

```{r elastic_net}
# specify regression model
model <- paste0('~ ', paste(metrics, collapse = ' + ')) %>% formula()
x <- model.matrix(model, data = data)

# majority LASSO
net1_cv <- cv.glmnet(x = x, y = data$win, alpha = .25, nfolds = 100)
# equal split
net2_cv <- cv.glmnet(x = x, y = data$win, alpha = .5, nfolds = 100)
# majority ridge
net3_cv <- cv.glmnet(x = x, y = data$win, alpha = .75, nfolds = 100)

# several predictors are leveraged
coef(net1_cv, s = net1_cv$lambda.min)
coef(net2_cv, s = net2_cv$lambda.min)
coef(net3_cv, s = net3_cv$lambda.min)

# all regression models produce similar results
# LASSO is most effective by a tiny margin
min(lasso_cv$cvm)
min(ridge_cv$cvm)
min(net1_cv$cvm)
min(net2_cv$cvm)
min(net3_cv$cvm)

# equal split most accurate
net_cv <- cv.glmnet(x = x, y = data$win, alpha = 0.5, nfolds = 100)
```

```{r test}
# train-test split
data_train <- data %>% sample_frac(.90)
data_test <- anti_join(data, data_train)
x_train <- model.matrix(model, data = data_train)

# fit the three candidate models
lasso_cv <- cv.glmnet(x = x_train, y = data_train$win, nfold = 100, alpha = 1)
ridge_cv <- cv.glmnet(x = x_train, y = data_train$win, nfold = 100, alpha = 0)
net_cv <- cv.glmnet(x = x_train, y = data_train$win, nfold = 100, alpha = .5)

# generate predictions for the test set
x_test <- model.matrix(model, data = data_test)
preds_lasso <- predict(lasso_cv, x_test, s = 'lambda.min')
preds_ridge <- predict(ridge_cv, x_test, s = 'lambda.min')
preds_net <- predict(net_cv, x_test, s = 'lambda.min')

# examine accurate matrix
cor(cbind(preds_lasso, preds_ridge, preds_net, data_test$win))

mean((preds_lasso - data_test$win)^2)
mean((preds_ridge - data_test$win)^2)
mean((preds_net - data_test$win)^2)
```

```{r win probability}
# assign predictors and outcome to formula variable
formula <- formula('factor(win) ~ win_pct_rank + adjoe_rank + adjde_rank + efg_o_rank + efg_d_rank + tor_rank + tord_rank + orb_rank + drb_rank + ftr_rank + ftrd_rank + twop_o_rank + twop_d_rank + threep_o_rank + threep_d_rank + adj_t_rank + win_pct_rank_opp + adjoe_rank_opp + adjde_rank_opp + efg_o_rank_opp + efg_d_rank_opp + tor_rank_opp + tord_rank_opp + orb_rank_opp + drb_rank_opp + ftr_rank_opp + ftrd_rank_opp + twop_o_rank_opp + twop_d_rank_opp + threep_o_rank_opp + threep_d_rank_opp + adj_t_rank_opp')

# assign all predictors to list
metrics <- c('win_pct_rank', 'adjoe_rank', 'adjde_rank', 'efg_o_rank', 'efg_d_rank', 'tor_rank', 'tord_rank', 'orb_rank', 'drb_rank', 'ftr_rank', 'ftrd_rank', 'twop_o_rank', 'twop_d_rank', 'threep_o_rank', 'threep_d_rank', 'adj_t_rank', 'win_pct_rank_opp', 'adjoe_rank_opp', 'adjde_rank_opp', 'efg_o_rank_opp', 'efg_d_rank_opp', 'tor_rank_opp', 'tord_rank_opp', 'orb_rank_opp', 'drb_rank_opp', 'ftr_rank_opp', 'ftrd_rank_opp', 'twop_o_rank_opp', 'twop_d_rank_opp', 'threep_o_rank_opp', 'threep_d_rank_opp', 'adj_t_rank_opp')

# specify training data 
model <- paste0('~ ', paste(metrics, collapse = ' + ')) %>% formula()
x <- model.matrix(model, data = data)
test <- model.matrix(model, data = season_test)

# fit the four candidate models
random_forest <- randomForest(formula, data = data, method = 'rf', ntree = 200, na.action = na.omit)
lasso_cv <- cv.glmnet(x = x, y = data$win, nfold = 100, alpha = 1)
ridge_cv <- cv.glmnet(x = x, y = data$win, nfold = 100, alpha = 0)
net_cv <- cv.glmnet(x = x, y = data$win, nfold = 100, alpha = .5)

# generate predictions
season_test$forest <- predict(random_forest, season_test)
season_test$forest <- as.numeric(season_test$forest) -1
season_test$lasso <- predict(lasso_cv, test, s = 'lambda.min')
season_test$lasso <- round(season_test$lasso, 0)
season_test$ridge <- predict(lasso_cv, test, s = 'lambda.min')
season_test$ridge <- round(season_test$ridge, 0)
season_test$net <- predict(lasso_cv, test, s = 'lambda.min')
season_test$net <- round(season_test$net, 0)

# calculate win probability of all 63 matchups
win_composite <- season_test %>% 
  group_by(year, team) %>% 
  summarize(forest = sum(forest),
            lasso = sum(lasso),
            ridge = sum(ridge),
            net = sum(net))
```
